# An egui app for prompting a local offline LLM.

<p align="center">
  <img alt="Example prompt" src="media/prompt.gif" height="450" width="370">
</p>

## Description

`coze` is a small [`egui`](https://github.com/emilk/egui) application for prompting
a local offline LLM using the Huggingface [`candle`](https://github.com/huggingface/candle)
crate.

Currently it supports the following quantized models:

- [StableLM 2 Zephyr 1.6B](https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b)
- [Mistral Instruct 7B v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)
- [Hugging Face Zephyr 7B Î²](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)

The current version supports:

- Prompt history navigation with fuzzy matching.
- History persistence across runs.
- Token generation modes.
- Copy prompts and replies to clipboard.
- Light/Dark mode.

See the app `Edit/Config` menu for usage details.

## Installation

The latest version of `coze` can be installed or updated with `cargo install`:

```sh
cargo install --locked coze
```

or by getting the binaries generated by the release Github action for Linux, macOS,
and Windows in the [releases page][github-releases].

[github-releases]: https://github.com/vincev/coze/releases/latest

The first time it runs it will download the model weights from Huggingface to the
`~/.cache/coze` folder.

[github-releases]: https://github.com/vincev/coze/releases/latest

To build locally (debug build may be very slow):

```bash
git clone https://github.com/vincev/coze
cd coze
cargo r --release
```
